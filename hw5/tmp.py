# -*- coding: utf-8 -*-
"""hw5_adv_examples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNxbaX4i7pxEF5c5GYc-CWX8TSJtipBL
"""

from torchvision.models import resnet50
import torch
from torch.autograd import Variable
import torch
import torch.nn
from torch.autograd.gradcheck import zero_gradients
import matplotlib.pyplot as plt
import json
import numpy as np
import torch.nn.functional as F
from torchvision.models import resnet50
from PIL import Image
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

model = resnet50(pretrained=True).eval()
myImage = Image.open('Elephant2.jpg')

#load the existing model
preprocess = transforms.Compose([transforms.Resize(224),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])
myTensor = preprocess(myImage)[None,:,:,:]

predVector = model(myTensor)
value, index = predVector.squeeze().max(0)
print(index.item())

#load the label index

with open('imagenet_class_index.json') as f:
    data = json.load(f)
labels_json = data
labels = {int(idx):label for idx, label in labels_json.items()}
x_pred = labels[index.item()]

print(x_pred)
image_tensor=myTensor
y_target = Variable(torch.LongTensor([12]), requires_grad=False)
epsilon = 0.3
num_steps = 5
alpha = 0.030
#hyper parameters
# i am using FGSM Methodology
img_variable = Variable(image_tensor, requires_grad=True)
img_variable.data = image_tensor   #in previous method we assigned it to the adversarial img

zero_gradients(img_variable) #flush all gradients
output = model(img_variable) #perform forward pass
loss = torch.nn.CrossEntropyLoss()
loss_cal = loss(output, y_target)
loss_cal.backward()
x_grad = alpha * torch.sign(img_variable.grad.data)
adv_temp = img_variable.data - x_grad
total_grad = adv_temp - image_tensor
total_grad = torch.clamp(total_grad, -epsilon, epsilon)
x_adv = image_tensor + total_grad  #add noise to img_variable
img_variable.data = x_adv
output_adv = model(img_variable)
value, index = output_adv.squeeze().max(0)
# x_adv_pred=labels[index.item()][1]

# print(x_adv_pred)

x=img_variable
x = x.squeeze(0)
x = x.mul(torch.FloatTensor([0.229, 0.224, 0.225]).view(3,1,1)).add(torch.FloatTensor([0.485, 0.456, 0.406]).view(3,1,1)).detach().numpy()
plt.imshow(np.transpose( x , (1,2,0)) )

image_tensor=myTensor;
data = myTensor
y_target = Variable(torch.LongTensor([466]), requires_grad=False)    #466= bullet_train
# epsilon = 0.3
# num_steps = 5
# alpha = 0.030
# #hyper parameters
# # i am using FGSM Methodology
img_variable = Variable(image_tensor, requires_grad=True)
img_variable.data = image_tensor   #in previous method we assigned it to the adversarial img
# for i in range(num_steps):
#   zero_gradients(img_variable) #flush all gradients
#   output = model(img_variable) #perform forward pass
#   loss = torch.nn.CrossEntropyLoss()
#   loss_cal = loss(output, y_target)
#   loss_cal.backward()
#   x_grad = alpha * torch.sign(img_variable.grad.data)
#   adv_temp = img_variable.data - x_grad
#   total_grad = adv_temp - image_tensor
#   total_grad = torch.clamp(total_grad, -epsilon, epsilon)
#   x_adv = image_tensor + total_grad  #add noise to img_variable
#   img_variable.data = x_adv
  
# output_adv = model(img_variable)
# value, index = output_adv.squeeze().max(0)
# # x_adv_pred=labels[index.item()][1]

# # x_adv_pred

# x=img_variable
# x = x.squeeze(0)
# x = x.mul(torch.FloatTensor([0.229, 0.224, 0.225]).view(3,1,1)).add(torch.FloatTensor([0.485, 0.456, 0.406]).view(3,1,1)).detach().numpy()
# plt.imshow(np.transpose( x , (1,2,0)) )
def where(cond, x, y):
    """
    code from :
        https://discuss.pytorch.org/t/how-can-i-do-the-operation-the-same-as-np-where/1329/8
    """
    cond = cond.float()
    return (cond*x) + ((1-cond)*y)

def i_fgsm(x, y, targeted=False, eps=0.2, alpha=0.03, iteration=35, x_val_min=0, x_val_max=1):
        x_adv = Variable(x.data, requires_grad=True)
        for i in range(iteration):
            h_adv = model(x_adv)
            if (h_adv.squeeze().max(0)[1]==y_target[0]).item():
                print("!!!!!!!! "+str(i))
                break
            if targeted:
                cost = F.cross_entropy(h_adv, y)
            else:
                cost = -F.cross_entropy(h_adv, y)

            model.zero_grad()
            if x_adv.grad is not None:
                x_adv.grad.data.fill_(0)
            cost.backward()

            x_grad = alpha * torch.sign(x_adv.grad.data)
            adv_temp = x_adv.data - x_grad
            total_grad = adv_temp - x
            total_grad = torch.clamp(total_grad, -epsilon, epsilon)
            x_adv2 = x + total_grad  #add noise to img_variable
            x_adv = Variable(x_adv2.data, requires_grad=True)

            # x_adv.grad.sign_()
            # x_adv = x_adv - alpha*x_adv.grad
            # x_adv = where(x_adv > x+eps, x+eps, x_adv)
            # x_adv = where(x_adv < x-eps, x-eps, x_adv)
            # x_adv = torch.clamp(x_adv, x_val_min, x_val_max)
            # x_adv = Variable(x_adv.data, requires_grad=True)

        h = model(x)
        h_adv = model(x_adv)

        return x_adv, h_adv, h

x_adv, h_adv, h = i_fgsm(img_variable, y_target, True)
print(h.squeeze().max(0)[1])
print(h_adv.squeeze().max(0)[1])

x = img_variable.squeeze(0)
x = x.mul(torch.FloatTensor([0.229, 0.224, 0.225]).view(3,1,1)).add(torch.FloatTensor([0.485, 0.456, 0.406]).view(3,1,1)).detach().numpy()
plt.imshow(np.transpose( x , (1,2,0)) )
plt.show()

x = x_adv.squeeze(0)
x = x.mul(torch.FloatTensor([0.229, 0.224, 0.225]).view(3,1,1)).add(torch.FloatTensor([0.485, 0.456, 0.406]).view(3,1,1)).detach().numpy()
plt.imshow(np.transpose( x , (1,2,0)) )
plt.show()