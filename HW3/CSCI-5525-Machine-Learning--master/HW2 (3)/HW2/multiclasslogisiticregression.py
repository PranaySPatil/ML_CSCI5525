# -*- coding: utf-8 -*-
"""multiclassLogisiticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QD26-gsxpfzmUE3P0fS3tCqnsiBVCpcR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pdb;

mnist_train=pd.read_csv('mnist_train.csv',header=None);
mnist_test=pd.read_csv('mnist_test.csv',header=None);
index = [i for i in range(mnist_train.shape[0])]
np.random.shuffle(index)
mnist_train=mnist_train.set_index([index]).sort_index()

y=mnist_train[0].values
x=mnist_train[mnist_train.columns.difference([0])].values
y_test =mnist_test[0].values
x_test=mnist_test[mnist_test.columns.difference([0])].values
x=(x-x.mean())/x.std()
x_test=(x_test-x_test.mean())/x_test.std()

n_r = 784
n_c = 10
w = np.random.randn(n_c, n_r)* 0.01
b = np.random.randn(n_c, 1)* 0.01

def softmax(z):
    #print(z)
    exp_vals = np.exp(z)
    probabilities = 1/sum(exp_vals) * exp_vals
    return probabilities

def loss_fun(predicted, expected):
    loss = -np.log(predicted[int(expected)])
    return loss

def mini_batch_gradient(w,b, x_batch, y_batch):
    batch_size = x_batch.shape[0] 
    w_grad_list = []
    b_grad_list = []
    batch_loss = 0
    for i in range(batch_size):
        x,y = x_batch[i],y_batch[i]
        x = x.reshape((784,1))
        E = np.zeros((10,1)) 
        E[y][0] = 1 
        predicted = softmax(np.dot(w, x)+b) #(10*1)

        loss = loss_fun(predicted, y)
        batch_loss += loss

        w_grad = E - predicted
        w_grad = - np.matmul(w_grad, x.reshape((1,784)))
        w_grad_list.append(w_grad)

        b_grad = -(E - predicted)
        b_grad_list.append(b_grad)

    dw = sum(w_grad_list)/batch_size
    db = sum(b_grad_list)/batch_size
    return dw, db, batch_loss

def multi_logistic_predict(w,b, x_data, y_data):
    loss_list = []
    pred_result=[]
    for i in range(len(y_data)):
      pred_result.append(softmax(np.dot(x_data[i], w.T)+b.mean()))

    result = np.argmax(np.array(pred_result),axis=1)
    accuracy = sum(result == y_data)/float(len(y_data))
    for i in range(len(y_data)):
      loss_list .append(loss_fun(pred_result[i],y_data[i]));
    loss = sum(loss_list)
    return loss, accuracy

def  multi_logistic_train(w,b,batch_size,learning_rate,x_train, y_train, x_test, y_test):
  num_batch = int(x_train.shape[0]/batch_size)
  rand_indices = np.random.choice(x_train.shape[0],x_train.shape[0],replace=False)
  for batch in range(num_batch):
    index = rand_indices[batch_size*batch:batch_size*(batch+1)]
    x_batch = x_train[index]
    y_batch = y_train[index]
    dw, db, batch_loss = mini_batch_gradient(w,b, x_batch, y_batch)        
    w -= learning_rate * dw
    b -= learning_rate * db
           
  return w,b;

def model(w,b, x_train, y_train, x_test, y_test):
  n_epochs = 15
  batch_size = 50
  learning_rate = 0.001
  train_loss_list=[]
  test_accu_list = []
  test_loss_list=[]
  train_accu_list = []
  for epoch in range(n_epochs):
    w,b=multi_logistic_train(w,b,batch_size,learning_rate, x_train, y_train, x_test, y_test);
    train_loss, train_accu = multi_logistic_predict(w,b,x_train,y_train)
    test_loss, test_accu = multi_logistic_predict(w,b,x_test,y_test)
    test_loss_list.append(test_loss)
    test_accu_list.append(test_accu)
    train_loss_list.append(train_loss)
    train_accu_list.append(train_accu)
  return test_loss_list, test_accu_list,w,train_loss_list,train_accu_list;

test_loss_list, test_accu_list,w,train_loss_list,train_accu_list = model(w,b,x,y,x_test,y_test)

len(test_loss_list)

test_accu_list

np.exp([1,2])/sum(np.exp([1,2]))

plt.plot(test_accu_list,label="Test")
#plt.plot(train_accu_list,label="Train")
plt.title("Accuracy-epochs ")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

#plt.plot(test_loss_list,label="Test")
plt.plot(train_loss_list,label="Train")
plt.title("Loss-epochs ")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

def compute_confusion_matrix(x, y, w, b):
    confusion_matrix = np.zeros((10,10))
    loss_list = []
    pred_result=[]
    for i in range(len(y)):
      pred_result.append(softmax(np.dot(x[i], w.T)+b.mean()))
    result = np.argmax(np.array(pred_result),axis=1)
    
    for i in range(len(y)):
      confusion_matrix[y[i]][result[i]] =  confusion_matrix[y[i]][result[i]] + 1      
    return confusion_matrix

print(compute_confusion_matrix(x_test, y_test, w, b).astype(int))

np.save('weights.npy', w)

