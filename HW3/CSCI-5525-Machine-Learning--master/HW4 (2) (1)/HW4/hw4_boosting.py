# -*- coding: utf-8 -*-
"""hw4_boosting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0ygGJFMoPQOobASRsiCt9MLCgOFtZgo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import test_score

#This method Generates Random vectors that are assumed to be the weak learners
def generate_random_predictor(n,p):
  if(p==0):
    k=pd.DataFrame(np.random.randint(0,2000,size=(n, 1)))
  else:
    #importance of different weak learner initialization with negative values:
    #if the value in the current ensemble is greater than or equal to the true value 
    k=pd.DataFrame(np.random.randint(-100,100,size=(n, 1)))

  return k.to_numpy();

def sigmoid(x):
  return 1/(1 + np.exp(-x)) ;

def compute(no_of_itr,prev,itr,prev_err):
  #Here i am assuming each weight are the sigmoid of 
  #print(x.max(axis=0))
  v=test_score.score(prev);
  sig=sigmoid(v);
  sig*=0.01;
  h=prev;
  min_score=v
  n=0
  i=0;
  train_error_list=[]
  while True:
    #weight normalization by the factor of itr/(itr+1);
    new_h=(generate_random_predictor(21283,itr)*sig+prev)*(itr/(itr+1));
    score=test_score.score(new_h)#[0];
    #choose the best fit by computing the error for specified number of random weak learners such that error will decrease
    if score<min_score and score<=v:
      min_score=score
      h=new_h;
    if min_score<v and n>no_of_itr:
      break;
    #if after the creation of 100 random weak learners the error does not decrease decrease the multiplyer
    if (n+1)%100==0:
      sig=sig/2
    if n>1000:
      print (score)
      print(v)
      print('break')
      break;
    n+=1 
  return h;

def model(no_of_trees):
  prev=np.zeros((21283, 1))
  prev_err=np.zeros((21283, 1))
  previous_vector=np.zeros((21283, 1))
  error_list=[];
  #create no of weak learners
  for  i in range(no_of_trees):
    prev_err=previous_vector;
    previous_vector=prev;
    prev=compute(100,prev,i,prev_err)
    error_list.append(test_score.score(prev))
    print(test_score.score(prev))
  return error_list

#plt.plot(no_of_tree_list,train_accuracy_collection)
train_accuracy_collection=model(10000)

plt.plot(train_accuracy_collection)
plt.title("Error  vs no of trees")
plt.ylabel("Error")
plt.xlabel("no of trees")

