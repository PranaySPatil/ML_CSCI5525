# -*- coding: utf-8 -*-
"""hw4randomforest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xGE8cfv0DW3jcgY0zZ2wDcbWZCb2Num1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree

data=pd.read_csv("Mushroom.csv",header=None);
#read the training data

def calculate_Accuracy(predicted_level,true_label):
  target=predicted_level.sum(axis=1)
  res=0
  for i in range(0,target.shape[0]):
    if target[i]*true_label[i]>0:
      res+=1;
  #comput e the total sum and predict based on the sign of sum
  return res*100/target.shape[0];

def model(trainset,x_test,y_test,no_of_features,max_depth,no_oftrees,sampling):
  train_predict_list=pd.DataFrame();
  test_predict_list=pd.DataFrame();
  #split the feature and label of the train data

  x_train=trainset;
  y_train=x_train[0];
  x_train=x_train.drop(0,axis=1)
  #run the loop to create the specified number of trees(weak learners)
  for i in range(no_oftrees):
    x_tr=pd.DataFrame();
    x_te=pd.DataFrame();
    x_train_with_features=pd.DataFrame();
    cnt=0;
    x_train_with_sampling=trainset.sample(frac=sampling,replace=False);
    #sampling with replacement
    y_train_with_sampling=x_train_with_sampling[0]
    x_train_with_sampling=x_train_with_sampling.drop(0,axis=1)
    
    for j in range(no_of_features):
      r=np.random.randint(1,22)#generate random number between 1 and 22 for a feature
      #print(j)
      x_train_with_features[cnt]=x_train[r];
      x_tr[cnt]=x_train_with_sampling[r];
      x_te[cnt]=x_test[r];
      cnt=cnt+1;
      #fit a weak learner into the model
    clf = tree.DecisionTreeClassifier(criterion="gini",max_depth=max_depth,splitter='random');
    clf = clf.fit(x_tr, y_train_with_sampling);
    train_predict=clf.predict(x_train_with_features);
    test_predict=clf.predict(x_te);
    #test_predict=clf.predict(x_test);
    train_predict_list[i]=train_predict;
    test_predict_list[i]=test_predict;
    

  return calculate_Accuracy(train_predict_list,y_train.to_numpy()),calculate_Accuracy(test_predict_list,y_test.to_numpy());

x_train=data.iloc[:6000];#choose first 6000 rows

x_test=data.iloc[6000:]

y_test=x_test[0]
x_test=x_test.drop(0,axis=1)

no_of_features_list=[5,10,15,20]
train_accuracy_collection=[]
test_accuracy_collection=[]
#call the model method for each features mentioned in the list
for i in no_of_features_list:
  train_accuracy,test_accuracy=model(x_train,x_test,y_test,i,2,100,1);
  train_accuracy_collection.append(train_accuracy)
  test_accuracy_collection.append(test_accuracy)

plt.plot(no_of_features_list,train_accuracy_collection)

plt.plot(no_of_features_list,test_accuracy_collection)
plt.legend(["train","test"])
plt.title("Accuracy  vs no of features")
plt.ylabel("Accuracy%")
plt.xlabel("no of features")

no_of_tree_list=[10,  20,  40,80,  100]
train_accuracy_collection=[]
test_accuracy_collection=[]
#call the model method for each noof trees mentioned in the list
for i in no_of_tree_list:
  train_accuracy,test_accuracy=model(x_train,x_test,y_test,22,2,i,1);
  train_accuracy_collection.append(train_accuracy)
  test_accuracy_collection.append(test_accuracy)

plt.figure()
plt.plot(no_of_tree_list,train_accuracy_collection)

plt.plot(no_of_tree_list,test_accuracy_collection)
plt.title("Accuracy  vs no of trees")
plt.legend(["train","test"])
plt.ylabel("Accuracy%")
plt.xlabel("no of trees")

