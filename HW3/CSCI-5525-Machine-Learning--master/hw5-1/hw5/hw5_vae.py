# -*- coding: utf-8 -*-
"""hw5_vae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AUQB-UFfw-tj9EJLpKNkSwTuUnn8j3rA
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch import nn, optim
from torch.autograd.variable import Variable
from torchvision import transforms, datasets
import torchvision.datasets as dsets
import torchvision.utils as vutils

train_data = dsets.MNIST(root='./data', 
                            train=True, 
                            transform=transforms.ToTensor(),
                            download=True)

test_data = dsets.MNIST(root='./data', 
                            train=False, 
                            transform=transforms.ToTensor(),
                            download=True)

class EncoderDecoderNet(torch.nn.Module):
    """
    A four hidden-layer Encoder_decoder neural network
    """
    def __init__(self):
        super(EncoderDecoderNet, self).__init__()
        n_features = 784
        n_out = 1
        
        self.hidden0 = nn.Sequential( 
            nn.Linear(n_features, 400),
            nn.ReLU(),
            
        )
        self.hidden1_mu = nn.Sequential(
            nn.Linear(400, 20),
            nn.ReLU(),
            
        )
        self.hidden1_sigma = nn.Sequential(
            nn.Linear(400, 20),
            nn.ReLU(),
            
        )
        self.hidden2 = nn.Sequential( 
            nn.Linear(20, 400),
            nn.ReLU(),
            
        )
        self.hidden3 = nn.Sequential(
            nn.Linear(400, n_features),
            nn.Sigmoid(),
        )
    def encoder(self, x):
      x = self.hidden0(x)
      return self.hidden1_mu(x), self.hidden1_sigma(x) # mu, log_var
    def decoder(self, x):
      x = self.hidden2(x)
      x = self.hidden3(x)
      return x;
    def sampling(self, mu, log_var):
        std = torch.exp(log_var)
        #eps = torch.randn_like(std)
        #used to introduce reparaeterisation trick
        eps = Variable(torch.FloatTensor(std.size()).normal_())
        return eps.mul(std).add_(mu)
   
    def forward(self, x):
        mu, log_var = self.encoder(x)
        z = self.sampling(mu, log_var)
        return self.decoder(z), mu, log_var

encoderDecoderNet = EncoderDecoderNet()

batch_size=64
train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=True)

optimizer = optim.Adam(encoderDecoderNet.parameters(),lr=0.001)
# Loss function
loss = nn.BCELoss(reduction='sum')

train_loss_list = []
num_epochs = 10
for epoch in range(num_epochs):
    current_loss = 0
    for image,label in train_data_loader:
        image_noisy = image;
        optimizer.zero_grad()
         # Reset gradients
        recon_batch, mu, log_var = encoderDecoderNet(image_noisy.reshape(image_noisy.size(0), -1))
        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())#compute Kl divergence between normal distribution and sampling Distribution
        # Calculate error and backpropagate
        loss_f = loss(recon_batch, image_noisy.view(-1, 784)) + KLD
        current_loss += loss_f.item()
        loss_f.backward()
        # Update weights with gradients
        optimizer.step()
        
    train_loss_list.append(current_loss/(len(train_data_loader)*64))
torch.save(encoderDecoderNet.state_dict(),"hw5_vae.pth")

plt.figure()
plt.plot(train_loss_list,label="loss")
plt.title("loss plot")
plt.xlabel("Epochs")
plt.ylabel("Error")

def vectors_to_images(vectors):
    return vectors.view(vectors.size(0), 1, 28, 28)

for image_test,label in test_data_loader:
        image_noisy_test = image_test

        output,_,_ = encoderDecoderNet(image_noisy_test.reshape(image_noisy_test.size(0), -1))
        image1 = image.reshape(image.size(0), -1)
         #predict the new image
        predicted_image = vectors_to_images(output)
        horizontal_grid = vutils.make_grid(
                  predicted_image, nrow=4, normalize=True, scale_each=True)
        fig = plt.figure(figsize=(4, 4))
        plt.imshow(np.moveaxis(horizontal_grid.detach().numpy(), 0, -1))
        #break the loop 
        break;

