# -*- coding: utf-8 -*-
"""hw3mnistfc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6OhD3X9W2vj0MuPkcifaLCXSleeF1o-
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
from torch.autograd import Variable

transform = transforms.Compose([transforms.ToTensor(),
  transforms.Normalize((0.5,), (0.5,))
])
trainset = torchvision.datasets.MNIST(root='./', train=True, download=True,transform=transform)
testset = torchvision.datasets.MNIST(root='./', train=False, download=True,transform=transform)

train_loader = torch.utils.data.DataLoader(trainset,batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(testset,batch_size=32, shuffle=True)

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
        

    def forward(self, x):
      x = F.relu(self.fc1(x))
      x = self.fc2(x)
      return F.log_softmax(x)


net = Net()

optimizer = optim.SGD(net.parameters(), lr=0.01)
    # create a loss function
criterion = nn.CrossEntropyLoss()
epochs=5
log_interval=10

losslist=[]
traing_accuracy_list=[]
for epoch in range(epochs):
  total_loss=0;
  training_accuracy=0;
  for batch_idx, (data, target) in enumerate(train_loader):
      data, target = Variable(data), Variable(target)
            # resize data 
      data = data.view(-1, 28*28)
      optimizer.zero_grad()
      net_out = net(data)
      loss = criterion(net_out, target)
      loss.backward()
       # Updating parameters
      optimizer.step()
       # Get predictions
      pred = net_out.data.max(1)[1]
      # Total correct predictions
      training_accuracy+=pred.eq(target.data).sum()

      total_loss+=loss;
  #print()
  losslist.append(total_loss.item()/60000)
  traing_accuracy_list.append(training_accuracy.item()/60000)

torch.save(net, "mnist-fc")
model = Net()
model = torch.load( "mnist-fc")

test_loss = 0
correct = 0
for data, target in test_loader:
    data, target = Variable(data, volatile=True), Variable(target)
    data = data.view(-1, 28 * 28)
    net_out = model(data)
        # sum up batch loss
    test_loss += criterion(net_out, target)
    pred = net_out.data.max(1)[1]  # get the index of the max log-probability
    correct += pred.eq(target.data).sum()

test_loss /= len(test_loader.dataset)
print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

import matplotlib.pyplot as plt

len(losslist)

plt.plot(losslist)
plt.title("SGD loss vs epochs")
plt.ylabel("loss%")
plt.xlabel("epochs")

plt.plot(traing_accuracy_list)
plt.title("SGD Accuracy vs epochs")
plt.ylabel("Accuracy %")
plt.xlabel("epochs")

